# è³‡æ–™é›†ç‰¹å¾µå‰µå»º

[![hackmd-github-sync-badge](https://hackmd.io/OF2FB_9mRY2nYiHrbnzmGg/badge)](https://hackmd.io/OF2FB_9mRY2nYiHrbnzmGg)


> æœ¬æ–‡ä»¶ä»‹ç´¹ä¸‰ç¨®ä¸åŒé¡å‹çš„ç‰¹å¾µæå–æ–¹æ³•ï¼Œç”¨æ–¼ç¶²å€é‡£é­šæª¢æ¸¬çš„è³‡æ–™é›†å»ºæ§‹

---

## ğŸ“‹ ç›®éŒ„

- [URL ç‰¹å¾µ](#url-ç‰¹å¾µ)
- [HTML ç‰¹å¾µ](#html-ç‰¹å¾µ)  
- [AI ç‰¹å¾µ](#ai-ç‰¹å¾µ)

---

## ğŸ”— URL ç‰¹å¾µ

### æ¦‚è¿°
ä½¿ç”¨ URL ç¶²å€è³‡è¨Šç²å–ç›¸é—œæ¬„ä½ï¼Œé€éåˆ†æç¶²å€çµæ§‹ã€å­—ç¬¦çµ„æˆç­‰ä¾†è­˜åˆ¥å¯ç–‘ç¶²å€ã€‚

### ğŸ“Š ç‰¹å¾µæ¬„ä½åˆ—è¡¨

#### åŸºæœ¬é•·åº¦ç‰¹å¾µ
- **length_url**: URL ç¸½é•·åº¦
- **length_hostname**: ä¸»æ©Ÿåç¨±é•·åº¦

#### å­—ç¬¦çµ±è¨ˆç‰¹å¾µ
- **ip**: æª¢æŸ¥ hostname æ˜¯å¦ç‚º IP åœ°å€
- **nb_dots**: é»è™Ÿæ•¸é‡
- **nb_hyphens**: é€£å­—è™Ÿæ•¸é‡
- **nb_at**: @ ç¬¦è™Ÿæ•¸é‡
- **nb_qm**: å•è™Ÿæ•¸é‡
- **nb_and**: & ç¬¦è™Ÿæ•¸é‡
- **nb_or**: | ç¬¦è™Ÿæ•¸é‡
- **nb_eq**: = ç¬¦è™Ÿæ•¸é‡
- **nb_underscore**: _ ç¬¦è™Ÿæ•¸é‡
- **nb_tilde**: ~ ç¬¦è™Ÿæ•¸é‡
- **nb_percent**: % ç¬¦è™Ÿæ•¸é‡
- **nb_slash**: / ç¬¦è™Ÿæ•¸é‡
- **nb_star**: * ç¬¦è™Ÿæ•¸é‡
- **nb_colon**: : ç¬¦è™Ÿæ•¸é‡
- **nb_comma**: , ç¬¦è™Ÿæ•¸é‡
- **nb_semicolumn**: ; ç¬¦è™Ÿæ•¸é‡
- **nb_dollar**: $ ç¬¦è™Ÿæ•¸é‡
- **nb_space**: ç©ºæ ¼æ•¸é‡

#### åŸŸåç‰¹å¾µ
- **nb_www**: æª¢æŸ¥æ˜¯å¦æœ‰ "www"
- **nb_com**: æª¢æŸ¥æ˜¯å¦æœ‰ ".com"
- **nb_dslash**: æª¢æŸ¥æ˜¯å¦æœ‰ "//" ä¸”ä¸åœ¨å”è­°éƒ¨åˆ†
- **http_in_path**: æª¢æŸ¥è·¯å¾‘ä¸­æ˜¯å¦æœ‰ "http" æˆ– "https"
- **https_token**: æª¢æŸ¥æ˜¯å¦ç‚º HTTPS
- **punycode**: æª¢æŸ¥æ˜¯å¦ç‚º Punycode ç·¨ç¢¼
- **port**: æª¢æŸ¥æ˜¯å¦æœ‰æŒ‡å®š Port

#### æ¯”ä¾‹ç‰¹å¾µ
- **ratio_digits_url**: URL ä¸­æ•¸å­—çš„æ¯”ä¾‹
- **ratio_digits_host**: hostname ä¸­æ•¸å­—çš„æ¯”ä¾‹

#### åŸŸåçµæ§‹ç‰¹å¾µ
- **tld_in_path**: é ‚ç´šåŸŸåæ˜¯å¦å‡ºç¾åœ¨è·¯å¾‘ä¸­
- **tld_in_subdomain**: é ‚ç´šåŸŸåæ˜¯å¦å‡ºç¾åœ¨å­åŸŸåä¸­
- **nb_subdomains**: å­åŸŸåæ•¸é‡
- **abnormal_subdomain**: å­åŸŸåæ˜¯å¦ç•°å¸¸
- **prefix_suffix**: æª¢æŸ¥åŸŸåæ˜¯å¦æœ‰å‰ç¶´æˆ–å¾Œç¶´ç¬¦è™Ÿ
- **path_extension**: æª¢æŸ¥è·¯å¾‘ä¸­æ˜¯å¦æœ‰æª”æ¡ˆå‰¯æª”å

#### æ–‡å­—ç‰¹å¾µ
- **length_words_raw**: URL ä¸­æ‰€æœ‰å–®è©çš„ç¸½é•·åº¦
- **char_repeat**: URL ä¸­æ˜¯å¦æœ‰é‡è¤‡å­—å…ƒåºåˆ—
- **shortest_word_host**: ä¸»æ©Ÿåç¨±ä¸­æœ€çŸ­å–®è©çš„é•·åº¦
- **shortest_word_path**: è·¯å¾‘ä¸­æœ€çŸ­å–®è©çš„é•·åº¦
- **longest_words_raw**: URL ä¸­æœ€é•·å–®è©çš„é•·åº¦
- **longest_word_host**: ä¸»æ©Ÿåç¨±ä¸­æœ€é•·å–®è©çš„é•·åº¦
- **longest_word_path**: è·¯å¾‘ä¸­æœ€é•·å–®è©çš„é•·åº¦
- **avg_words_raw**: URL ä¸­å–®è©çš„å¹³å‡é•·åº¦
- **avg_word_host**: ä¸»æ©Ÿåç¨±ä¸­å–®è©çš„å¹³å‡é•·åº¦
- **avg_word_path**: è·¯å¾‘ä¸­å–®è©çš„å¹³å‡é•·åº¦

### ğŸ’» ç¨‹å¼ç¢¼å¯¦ä½œ

```python
import pandas as pd
from urllib.parse import urlparse
import ipaddress
import re

def extract_url_features(df: pd.DataFrame, url_column: str = 'url') -> pd.DataFrame:
    """
    åœ¨ DataFrame ä¸­ç‚ºæŒ‡å®šçš„ URL æ¬„ä½å‰µå»ºå¤šå€‹ç‰¹å¾µã€‚

    Args:
        df (pd.DataFrame): åŒ…å« URL æ¬„ä½çš„ DataFrameã€‚
        url_column (str): DataFrame ä¸­åŒ…å« URL çš„æ¬„ä½åç¨±ã€‚é è¨­ç‚º 'url'ã€‚

    Returns:
        pd.DataFrame: åŒ…å«æ–°å‰µå»ºç‰¹å¾µæ¬„ä½çš„ DataFrameã€‚
    """

    if url_column not in df.columns:
        raise ValueError(f"DataFrame ä¸­æœªæ‰¾åˆ°æŒ‡å®šçš„ URL æ¬„ä½: '{url_column}'")

    # ç¢ºä¿æ‰€æœ‰ URL éƒ½æ˜¯å­—ä¸²é¡å‹ï¼Œä¸¦è™•ç†å¯èƒ½çš„ NaN
    df[url_column] = df[url_column].astype(str).replace('nan', '')

    results = []
    word_split_pattern = re.compile(r'[^a-zA-Z0-9]+')
    
    for index, row in df.iterrows():
        url = row[url_column]
        features = {}

        if not url: # å¦‚æœ URL æ˜¯ç©ºå­—ä¸²ï¼Œå‰‡æ‰€æœ‰ç‰¹å¾µéƒ½ç‚º 0 æˆ–é è¨­å€¼
            results.append({col: 0 for col in feature_columns})
            continue

        try:
            parsed_url = urlparse(url)
            scheme = parsed_url.scheme
            hostname = parsed_url.hostname if parsed_url.hostname else ''
            path = parsed_url.path
            query = parsed_url.query
            fragment = parsed_url.fragment

            # 1. length_url (URL é•·åº¦)
            features['length_url'] = len(url)

            # 2. length_hostname (hostname é•·åº¦)
            features['length_hostname'] = len(hostname)

            # 3. ip (æª¢æŸ¥ hostname æ˜¯å¦ç‚º IP åœ°å€)
            features['ip'] = 0
            if hostname:
                try:
                    ipaddress.ip_address(hostname)
                    features['ip'] = 1
                except ValueError:
                    pass # ä¸æ˜¯æœ‰æ•ˆçš„ IP åœ°å€

            # 4. nb_dots (é»è™Ÿæ•¸é‡)
            features['nb_dots'] = hostname.count('.')

            # 5. nb_hyphens (é€£å­—è™Ÿæ•¸é‡)
            features['nb_hyphens'] = url.count('-')

            # 6. nb_at (@ ç¬¦è™Ÿæ•¸é‡)
            features['nb_at'] = url.count('@')

            # 7. nb_qm (å•è™Ÿæ•¸é‡)
            features['nb_qm'] = url.count('?')

            # 8. nb_and (& ç¬¦è™Ÿæ•¸é‡)
            features['nb_and'] = url.count('&')

            # 9. nb_or (| ç¬¦è™Ÿæ•¸é‡) - è¼ƒå°‘è¦‹ï¼Œä½†ä»æª¢æŸ¥
            features['nb_or'] = url.count('|')

            # 10. nb_eq (= ç¬¦è™Ÿæ•¸é‡)
            features['nb_eq'] = url.count('=')

            # 11. nb_underscore (_ ç¬¦è™Ÿæ•¸é‡)
            features['nb_underscore'] = url.count('_')

            # 12. nb_tilde (~ ç¬¦è™Ÿæ•¸é‡)
            features['nb_tilde'] = url.count('~')

            # 13. nb_percent (% ç¬¦è™Ÿæ•¸é‡)
            features['nb_percent'] = url.count('%')

            # 14. nb_slash (/ ç¬¦è™Ÿæ•¸é‡)
            features['nb_slash'] = url.count('/')

            # 15. nb_star (* ç¬¦è™Ÿæ•¸é‡)
            features['nb_star'] = url.count('*')

            # 16. nb_colon (: ç¬¦è™Ÿæ•¸é‡)
            features['nb_colon'] = url.count(':')

            # 17. nb_comma (, ç¬¦è™Ÿæ•¸é‡)
            features['nb_comma'] = url.count(',')

            # 18. nb_semicolumn (; ç¬¦è™Ÿæ•¸é‡)
            features['nb_semicolumn'] = url.count(';')

            # 19. nb_dollar ($ ç¬¦è™Ÿæ•¸é‡)
            features['nb_dollar'] = url.count('$')

            # 20. nb_space (ç©ºæ ¼æ•¸é‡) - é€šå¸¸ URL ä¸æ‡‰è©²æœ‰ç©ºæ ¼ï¼Œä½†ç‚ºäº†é­¯æ£’æ€§ä»æª¢æŸ¥
            features['nb_space'] = url.count(' ')

            # 21. nb_www (æª¢æŸ¥æ˜¯å¦æœ‰ "www")
            features['nb_www'] = 1 if 'www' in hostname.lower() else 0

            # 22. nb_com (æª¢æŸ¥æ˜¯å¦æœ‰ ".com") - é€™è£¡åªæª¢æŸ¥ hostname
            features['nb_com'] = 1 if '.com' in hostname.lower() else 0

            # 23. nb_dslash (æª¢æŸ¥æ˜¯å¦æœ‰ "//" ä¸”ä¸åœ¨å”è­°éƒ¨åˆ†)
            features['nb_dslash'] = 1 if '//' in path + query + fragment else 0

            # 24. http_in_path (æª¢æŸ¥è·¯å¾‘ä¸­æ˜¯å¦æœ‰ "http" æˆ– "https")
            features['http_in_path'] = 1 if re.search(r'http[s]?://', path + query + fragment, re.IGNORECASE) else 0

            # 25. https_token (æª¢æŸ¥æ˜¯å¦ç‚º HTTPS)
            features['https_token'] = 1 if scheme == 'https' else 0

            # 26. ratio_digits_url (URL ä¸­æ•¸å­—çš„æ¯”ä¾‹)
            digits_in_url = sum(c.isdigit() for c in url)
            features['ratio_digits_url'] = digits_in_url / len(url) if len(url) > 0 else 0

            # 27. ratio_digits_host (hostname ä¸­æ•¸å­—çš„æ¯”ä¾‹)
            digits_in_host = sum(c.isdigit() for c in hostname)
            features['ratio_digits_host'] = digits_in_host / len(hostname) if len(hostname) > 0 else 0

            # 28. punycode (æª¢æŸ¥æ˜¯å¦ç‚º Punycode ç·¨ç¢¼)
            features['punycode'] = 1 if hostname.startswith('xn--') else 0

            # 29. port (æª¢æŸ¥æ˜¯å¦æœ‰æŒ‡å®š Port)
            features['port'] = 1 if parsed_url.port else 0

            # 30. tld_in_path (é ‚ç´šåŸŸåæ˜¯å¦å‡ºç¾åœ¨è·¯å¾‘ä¸­)
            tld = hostname.split('.')[-1] if '.' in hostname else ''
            features['tld_in_path'] = 1 if tld and tld in path.lower() else 0

            # 31. tld_in_subdomain (é ‚ç´šåŸŸåæ˜¯å¦å‡ºç¾åœ¨å­åŸŸåä¸­)
            subdomains = hostname.split('.')[:-1] if '.' in hostname else []
            features['tld_in_subdomain'] = 0
            if tld:
                for sub in subdomains:
                    if tld in sub.lower():
                        features['tld_in_subdomain'] = 1
                        break

            # 32. nb_subdomains (å­åŸŸåæ•¸é‡)
            if features['ip'] == 1: # IP åœ°å€æ²’æœ‰å­åŸŸåæ¦‚å¿µ
                features['nb_subdomains'] = 0
            elif hostname:
                parts = hostname.split('.')
                features['nb_subdomains'] = hostname.count('.')
                if hostname.count('.') >= 1:
                    features['nb_subdomains'] = hostname.count('.') - 1
                    if 'www' in parts[0].lower():
                        features['nb_subdomains'] -= 1
                    features['nb_subdomains'] = max(0, features['nb_subdomains'])
            else:
                features['nb_subdomains'] = 0

            # 33. abnormal_subdomain (å­åŸŸåæ˜¯å¦ç•°å¸¸)
            features['abnormal_subdomain'] = 0
            if features['nb_subdomains'] > 2:
                pass

            # 34. prefix_suffix (æª¢æŸ¥åŸŸåæ˜¯å¦æœ‰å‰ç¶´æˆ–å¾Œç¶´ç¬¦è™Ÿ)
            features['prefix_suffix'] = 0
            if hostname:
                if '-' in hostname and not hostname.startswith('-') and not hostname.endswith('-'):
                    features['prefix_suffix'] = 1

            # 35. path_extension (æª¢æŸ¥è·¯å¾‘ä¸­æ˜¯å¦æœ‰æª”æ¡ˆå‰¯æª”å)
            features['path_extension'] = 0
            if path:
                match = re.search(r'\.([a-zA-Z0-9]+)$', path)
                if match:
                    features['path_extension'] = 1

            # 36. length_words_raw: URL ä¸­æ‰€æœ‰å–®è©çš„ç¸½é•·åº¦
            all_url_words = [word for word in word_split_pattern.split(url) if word]
            features['length_words_raw'] = sum(len(word) for word in all_url_words)

            # 37. char_repeat: URL ä¸­æ˜¯å¦æœ‰é‡è¤‡å­—å…ƒåºåˆ—
            features['char_repeat'] = 0
            if re.search(r'(.)\1{1,}', url):
                features['char_repeat'] = 1

            # æå– hostname å’Œ path ä¸­çš„å–®è©
            hostname_words = [word for word in word_split_pattern.split(hostname) if word]
            path_query_fragment = path + query + fragment
            path_words = [word for word in word_split_pattern.split(path_query_fragment) if word]

            # 38. shortest_word_host: ä¸»æ©Ÿåç¨±ä¸­æœ€çŸ­å–®è©çš„é•·åº¦
            features['shortest_word_host'] = min(len(word) for word in hostname_words) if hostname_words else 0

            # 39. shortest_word_path: è·¯å¾‘ä¸­æœ€çŸ­å–®è©çš„é•·åº¦
            features['shortest_word_path'] = min(len(word) for word in path_words) if path_words else 0

            # 40. longest_words_raw: URL ä¸­æœ€é•·å–®è©çš„é•·åº¦
            features['longest_words_raw'] = max(len(word) for word in all_url_words) if all_url_words else 0

            # 41. longest_word_host: ä¸»æ©Ÿåç¨±ä¸­æœ€é•·å–®è©çš„é•·åº¦
            features['longest_word_host'] = max(len(word) for word in hostname_words) if hostname_words else 0

            # 42. longest_word_path: è·¯å¾‘ä¸­æœ€é•·å–®è©çš„é•·åº¦
            features['longest_word_path'] = max(len(word) for word in path_words) if path_words else 0

            # 43. avg_words_raw: URL ä¸­å–®è©çš„å¹³å‡é•·åº¦
            features['avg_words_raw'] = float(features['length_words_raw']) / len(all_url_words) if all_url_words else 0.0

            # 44. avg_word_host: ä¸»æ©Ÿåç¨±ä¸­å–®è©çš„å¹³å‡é•·åº¦
            features['avg_word_host'] = float(sum(len(word) for word in hostname_words)) / len(hostname_words) if hostname_words else 0.0

            # 45. avg_word_path: è·¯å¾‘ä¸­å–®è©çš„å¹³å‡é•·åº¦
            features['avg_word_path'] = float(sum(len(word) for word in path_words)) / len(path_words) if path_words else 0.0

        except Exception as e:
            # è™•ç†è§£æ URL æ™‚å¯èƒ½ç™¼ç”Ÿçš„éŒ¯èª¤
            print(f"è™•ç† URL '{url}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            features = {col: 0.0 for col in feature_columns}

        results.append(features)

    # å°‡çµæœè½‰æ›ç‚º DataFrame
    features_df = pd.DataFrame(results, index=df.index)

    # å°‡æ–°ç‰¹å¾µåˆä½µåˆ°åŸå§‹ DataFrame
    df_with_features = pd.concat([df, features_df], axis=1)

    return df_with_features

# ç¯„ä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # å‘¼å«å‡½å¼å‰µå»ºæ–°ç‰¹å¾µ
    df_with_new_features = extract_url_features(df, url_column='url')
    
    print("åŒ…å«æ–°ç‰¹å¾µçš„ DataFrame:")
    print(df_with_new_features.head())
    
    print("\næ‰€æœ‰æ–°å‰µå»ºçš„ç‰¹å¾µæ¬„ä½åŠå…¶å€¼é¡å‹:")
    for col in [f for f in df_with_new_features.columns if f not in df.columns]:
        print(f"- {col}: {df_with_new_features[col].dtype}")
```

---

## ğŸŒ HTML ç‰¹å¾µ

### æ¦‚è¿°
ä½¿ç”¨ HTML ç¶²é æ–‡æœ¬ç²å–ç›¸é—œæ¬„ä½ï¼Œé€éåˆ†æç¶²é å…§å®¹ã€é€£çµçµæ§‹ã€è½‰å‘è¡Œç‚ºç­‰ä¾†è­˜åˆ¥å¯ç–‘ç¶²ç«™ã€‚

### ğŸ“Š ç‰¹å¾µæ¬„ä½åˆ—è¡¨

#### è½‰å‘æª¢æ¸¬ç‰¹å¾µ
- **has_meta_refresh**: åµæ¸¬ meta è½‰å‘
- **has_js_redirect**: åµæ¸¬ JavaScript è½‰å‘

#### å…§å®¹åˆ†æç‰¹å¾µ
- **phish_hints**: HTML å…§å®¹ä¸­æ˜¯å¦å­˜åœ¨å¸¸è¦‹çš„é‡£é­šæç¤ºè©èª
- **domain_in_brand**: ç¶²ç«™å…§å®¹ä¸­æåŠçš„å“ç‰Œåç¨±æ˜¯å¦èˆ‡åŸŸåä¸€è‡´
- **empty_title**: ç¶²é æ¨™é¡Œæ˜¯å¦ç‚ºç©º
- **domain_in_title**: åŸŸåæ˜¯å¦å‡ºç¾åœ¨ç¶²é æ¨™é¡Œä¸­
- **domain_with_copyright**: ç¶²ç«™çš„ç‰ˆæ¬Šè³‡è¨Šä¸­æ˜¯å¦åŒ…å«åŸŸå

#### é€£çµåˆ†æç‰¹å¾µ
- **nb_hyperlinks**: ç¶²é ä¸­è¶…é€£çµçš„ç¸½æ•¸
- **ratio_intHyperlinks**: å…§éƒ¨è¶…é€£çµçš„æ¯”ä¾‹
- **ratio_extHyperlinks**: å¤–éƒ¨è¶…é€£çµçš„æ¯”ä¾‹
- **ratio_extRedirection**: å¤–éƒ¨é‡æ–°å°å‘çš„æ¯”ä¾‹
- **ratio_extErrors**: å¤–éƒ¨é€£çµä¸­è¿”å›éŒ¯èª¤çš„æ¯”ä¾‹
- **safe_anchor**: éŒ¨é»é€£çµæ˜¯å¦å®‰å…¨ï¼ˆä¾‹å¦‚é¿å…æŒ‡å‘å¯ç–‘å¤–éƒ¨ç¶²ç«™ï¼‰

#### è³‡æºåˆ†æç‰¹å¾µ
- **external_favicon**: ç¶²ç«™æ˜¯å¦ä½¿ç”¨ä¾†è‡ªå¤–éƒ¨åŸŸåçš„ Favicon
- **links_in_tags**: ç‰¹å®š HTML æ¨™ç±¤ï¼ˆå¦‚ `<a>`ã€`<script>`ï¼‰ä¸­é€£çµçš„æ•¸é‡
- **ratio_extMedia**: å¤–éƒ¨åª’é«”ï¼ˆåœ–ç‰‡ã€éŸ³é »ã€è¦–é »ï¼‰çš„æ¯”ä¾‹

#### ç‹€æ…‹ç‰¹å¾µ
- **feature_extracted**: ç‰¹å¾µæ˜¯å¦æˆåŠŸæå–çš„æ¨™è¨˜

### ğŸ› ï¸ ç’°å¢ƒè¨­å®š

```bash
# å®‰è£å¿…è¦å¥—ä»¶
pip install google-generativeai
pip install selenium
pip install webdriver-manager
apt-get update
apt install chromium-chromedriver

# æ‰‹å‹•å®‰è£èˆ‡æœ€æ–° WebDriver ç›¸ç¬¦çš„ Chrome ç€è¦½å™¨
wget -q https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/114.0.5735.90/linux64/chrome-linux64.zip
unzip -q chrome-linux64.zip -d /bin/
```

### ğŸ’» ç¨‹å¼ç¢¼å¯¦ä½œ

```python
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import re
import time
from collections import Counter
import ipaddress
import signal
from tqdm import tqdm
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

def get_html_content(url, timeout=20, max_retries=2):
    """
    æ ¹æ“š URL ç²å–ç¶²é çš„å®Œæ•´ HTML å…§å®¹ï¼Œä¸¦åµæ¸¬æ˜¯å¦æœ‰è½‰å‘ã€‚
    å…ˆä½¿ç”¨ requests çˆ¬å–ï¼Œå¦‚æœå…§å®¹ç‚ºç©ºæˆ–å¤±æ•—å‰‡ä½¿ç”¨ Seleniumã€‚
    
    Args:
        url (str): ç›®æ¨™ç¶²é çš„ URLã€‚
        timeout (int): è«‹æ±‚è¶…æ™‚æ™‚é–“ (ç§’)ã€‚
        max_retries (int): å¤±æ•—æ™‚é‡è©¦æ¬¡æ•¸ã€‚
    
    Returns:
        tuple[str, bool]: ç¶²é çš„ HTML å…§å®¹ï¼Œå¦‚æœç²å–å¤±æ•—å‰‡è¿”å› Noneã€‚
                         ä»¥åŠä¸€å€‹å¸ƒæ—å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ç™¼ç”Ÿäº†è½‰å‘ã€‚
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.7258.66 Safari/537.36'
    }

    # ç¬¬ä¸€æ­¥ï¼šå˜—è©¦ä½¿ç”¨ requests çˆ¬å–
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser')
            body_content = soup.find('body')
            
            # æª¢æŸ¥å…§å®¹æ˜¯å¦ç‚ºç©ºæˆ–éçŸ­ï¼ˆå¯èƒ½æ˜¯å‹•æ…‹å…§å®¹ï¼‰
            if html_content and len(html_content.strip()) > 100:
                # æª¢æŸ¥é é¢å…§å®¹æ˜¯å¦åŒ…å« "page not found" ç›¸é—œé—œéµè©
                not_found_keywords = ['page not found', 'error 404', 'page does not exist', 'æ‰¾ä¸åˆ°é é¢', 'é é¢ä¸å­˜åœ¨']
                matched_keywords = 0
                try:
                    body_text = body_content.get_text(strip=True)
                    body_lower = body_text.lower()
                    matched_keywords = sum(1 for keyword in not_found_keywords if keyword in body_lower)
                except:
                    html_lower = html_content.lower()
                    matched_keywords = sum(1 for keyword in not_found_keywords if keyword in html_lower)
                
                if matched_keywords >= 2:
                    print(f"âŒ URL {url} é é¢å…§å®¹åŒ…å« {matched_keywords} å€‹ 'page not found' ç›¸é—œé—œéµè©ï¼Œç›´æ¥è·³é")
                    return None, False

                print(f"âœ… ä½¿ç”¨ requests æˆåŠŸç²å– {url} çš„å…§å®¹")
                return html_content, False
            else:
                print(f"âš ï¸  requests ç²å–çš„å…§å®¹ç‚ºç©ºæˆ–éçŸ­ï¼Œå°‡å˜—è©¦ä½¿ç”¨ Selenium")
                break

        except requests.exceptions.HTTPError as e:
            if response.status_code == 404:
                print(f"âŒ URL {url} è¿”å› 404 éŒ¯èª¤ï¼Œç›´æ¥è·³é")
                return None, False
            elif response.status_code >= 400:
                print(f"âŒ URL {url} è¿”å› HTTP {response.status_code} éŒ¯èª¤ï¼Œç›´æ¥è·³é")
                return None, False
            else:
                print(f"[é‡è©¦ {attempt+1}/{max_retries}] Requests ç²å– {url} æ™‚ç™¼ç”Ÿ HTTP éŒ¯èª¤: {e}")
                if attempt == max_retries:
                    print(f"âŒ Requests é‡è©¦ {max_retries} æ¬¡å¾Œä»ç„¶å¤±æ•—ï¼Œå°‡å˜—è©¦ä½¿ç”¨ Selenium")
                    break
                time.sleep(0.5)
        except requests.exceptions.RequestException as e:
            print(f"[é‡è©¦ {attempt+1}/{max_retries}] Requests ç²å– {url} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            if attempt == max_retries:
                print(f"âŒ Requests é‡è©¦ {max_retries} æ¬¡å¾Œä»ç„¶å¤±æ•—ï¼Œå°‡å˜—è©¦ä½¿ç”¨ Selenium")
                break
            time.sleep(1)

    # ç¬¬äºŒæ­¥ï¼šå¦‚æœ requests å¤±æ•—æˆ–å…§å®¹ç‚ºç©ºï¼Œä½¿ç”¨ Selenium
    print(f"ğŸ”„ é–‹å§‹ä½¿ç”¨ Selenium çˆ¬å– {url},æš«æ™‚è¨­ç‚ºå¤±æ•—")
    return _fetch_dynamic_content(url)

def _fetch_dynamic_content(url: str):
    """
    ä½¿ç”¨ Selenium çˆ¬å–å‹•æ…‹ç¶²é å…§å®¹ã€‚
    """
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-setuid-sandbox')
        chrome_options.add_argument('--user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"')
        chrome_options.binary_location = "/bin/chrome-linux64/chrome"
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)

        driver.get(url)
        time.sleep(10)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        web_text = soup.get_text(separator=' ', strip=True)

        print(f"ç¶²é  {url} å‹•æ…‹çˆ¬å–å®Œæˆ...")
        return (web_text, 'OK_Dynamic') if web_text else (None, 'OK_Dynamic_Empty')
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: ä½¿ç”¨ Selenium çˆ¬å–ç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚è¨Šæ¯: {e}")
        return None, 'Error_Selenium'
    finally:
        if 'driver' in locals():
            driver.quit()

def extract_html_features(df: pd.DataFrame, url_column: str = 'url') -> pd.DataFrame:
    """
    åœ¨ DataFrame ä¸­ç‚ºæŒ‡å®šçš„ URL æ¬„ä½çˆ¬å– HTML å…§å®¹ä¸¦å‰µå»ºå¤šå€‹ç‰¹å¾µã€‚
    æœƒè‡ªå‹•å…ˆä½¿ç”¨ requests çˆ¬å–ï¼Œå¦‚æœå…§å®¹ç‚ºç©ºå‰‡ä½¿ç”¨ Seleniumã€‚

    Args:
        df (pd.DataFrame): åŒ…å« URL æ¬„ä½çš„ DataFrameã€‚
        url_column (str): DataFrame ä¸­åŒ…å« URL çš„æ¬„ä½åç¨±ã€‚é è¨­ç‚º 'url'ã€‚

    Returns:
        pd.DataFrame: åŒ…å«æ–°å‰µå»ºç‰¹å¾µæ¬„ä½çš„ DataFrameã€‚
    """

    if url_column not in df.columns:
        raise ValueError(f"DataFrame ä¸­æœªæ‰¾åˆ°æŒ‡å®šçš„ URL æ¬„ä½: '{url_column}'")

    # ç¢ºä¿æ‰€æœ‰ URL éƒ½æ˜¯å­—ä¸²é¡å‹ï¼Œä¸¦è™•ç†å¯èƒ½çš„ NaN
    df[url_column] = df[url_column].astype(str).replace('nan', '')

    # æ›´æ–°ç‰¹å¾µæ¬„ä½åˆ—è¡¨ï¼ŒåŠ å…¥å…©å€‹è½‰å‘ç‰¹å¾µ
    html_feature_columns = [
        'phish_hints', 'domain_in_brand', 'nb_hyperlinks', 'ratio_intHyperlinks',
        'ratio_extHyperlinks', 'ratio_extRedirection', 'ratio_extErrors',
        'external_favicon', 'links_in_tags', 'ratio_extMedia', 'safe_anchor',
        'empty_title', 'domain_in_title', 'domain_with_copyright',
        'has_meta_refresh', 'has_js_redirect',
        'feature_extracted'
    ]

    results = []

    for index, row in df.iterrows():
        url = row[url_column]
        features = {col: 0.0 for col in html_feature_columns}

        if not url:
            features['feature_extracted'] = 0.0
            results.append(features)
            continue

        print(f"æ­£åœ¨è™•ç†ç¬¬ {index+1} / {len(df)} ç­† URL: {url}")

        # è¨­å®šå–®å€‹ URL çš„æœ€å¤§è™•ç†æ™‚é–“ï¼ˆç§’ï¼‰
        max_url_time = 30

        try:
            # å‘¼å« get_html_content ä¸¦æ¥æ”¶å…©å€‹è¿”å›å€¼
            html_content, has_js_redirect = safe_process_url(url, max_time=120)
        except Exception as e:
            print(f"âŒ è™•ç† URL {url} æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            features['feature_extracted'] = 0.0
            results.append(features)
            continue

        if html_content:
            try:
                soup = BeautifulSoup(html_content, 'html.parser')
                parsed_url = urlparse(url)
                base_domain = parsed_url.netloc.split(':')[0]
                if base_domain.startswith('www.'):
                    base_domain = base_domain[4:]
                redirect_keywords = ["window.location.href"]
                
                # åµæ¸¬ meta è½‰å‘
                meta_refresh_tag = soup.find('meta', attrs={'http-equiv': lambda x: x and x.lower() == 'refresh'})
                if meta_refresh_tag:
                    content = meta_refresh_tag.get("content", "")
                    features['has_meta_refresh'] = 1.0 if "url=" in content.lower() else 0.0
                else:
                    features['has_meta_refresh'] = 0.0

                # åµæ¸¬ JavaScript è½‰å‘
                features['has_js_redirect'] = 1.0 if soup.find("script", string=lambda s: any(k in s for k in redirect_keywords) if s else False) else 0.0

                # phish_hints: HTML å…§å®¹ä¸­æ˜¯å¦å­˜åœ¨å¸¸è¦‹çš„é‡£é­šæç¤ºè©èª
                phish_keywords = ['login', 'signin', 'account update', 'verify account',
                                  'security alert', 'password', 'bank', 'paypal', 'credit card',
                                  'ç·Šæ€¥', 'é©—è­‰', 'ç™»å…¥', 'å¸³æˆ¶æ›´æ–°', 'å®‰å…¨è­¦å‘Š', 'å¯†ç¢¼']
                text_content = soup.get_text().lower()
                features['phish_hints'] = 1 if any(kw in text_content for kw in phish_keywords) else 0.0

                # domain_in_brand: ç¶²ç«™å…§å®¹ä¸­æåŠçš„å“ç‰Œåç¨±æ˜¯å¦èˆ‡åŸŸåä¸€è‡´
                brand_match = 0
                domain_parts = base_domain.split('.')
                if len(domain_parts) >= 2:
                    tld = domain_parts[-1]
                    if tld in ['com', 'org', 'net', 'edu', 'gov', 'mil']:
                        core_domain = domain_parts[-2]
                    else:
                        core_domain = domain_parts[-3] if len(domain_parts) >= 3 else domain_parts[-2]
                else:
                    core_domain = domain_parts[0]

                title_tag = soup.find('title')
                if title_tag and core_domain in title_tag.get_text().lower():
                    brand_match = 1
                elif soup.find('meta', attrs={'name': 'description'}) and core_domain in soup.find('meta', attrs={'name': 'description'})['content'].lower():
                    brand_match = 1
                features['domain_in_brand'] = brand_match

                # nb_hyperlinks: ç¶²é ä¸­è¶…é€£çµçš„ç¸½æ•¸
                all_links = soup.find_all('a', href=True)
                features['nb_hyperlinks'] = len(all_links)

                # ratio_intHyperlinks: å…§éƒ¨è¶…é€£çµçš„æ¯”ä¾‹
                # ratio_extHyperlinks: å¤–éƒ¨è¶…é€£çµçš„æ¯”ä¾‹
                internal_links = 0
                external_links = 0
                for link_tag in all_links:
                    href = link_tag['href']
                    full_url = urljoin(url, href)
                    linked_domain = urlparse(full_url).netloc
                    if linked_domain == parsed_url.netloc:
                        internal_links += 1
                    else:
                        external_links += 1
                total_links_calc = internal_links + external_links
                features['ratio_intHyperlinks'] = internal_links / total_links_calc if total_links_calc > 0 else 0.0
                features['ratio_extHyperlinks'] = external_links / total_links_calc if total_links_calc > 0 else 0.0

                # ratio_extRedirection (å¤–éƒ¨é‡æ–°å°å‘çš„æ¯”ä¾‹)
                redirect_count = 0
                for link_tag in all_links:
                    href = link_tag['href']
                    if href.startswith('#'):
                        continue
                    full_url = urljoin(url, href)
                    linked_parsed = urlparse(full_url)
                    linked_domain = linked_parsed.netloc
                    is_external = (linked_domain != parsed_url.netloc)

                    if is_external:
                        if link_tag.get('onclick') and 'window.location' in link_tag.get('onclick', ''):
                            redirect_count += 1
                        elif link_tag.get('target') == '_blank' and 'redirect' in link_tag.get_text().lower():
                            redirect_count += 1

                features['ratio_extRedirection'] = redirect_count / len(all_links) if all_links else 0.0

                # ratio_extErrors (å¤–éƒ¨é€£çµä¸­è¿”å›éŒ¯èª¤çš„æ¯”ä¾‹)
                error_count = 0
                for link_tag in all_links:
                    href = link_tag['href']
                    if href.startswith('#'):
                        continue
                    full_url = urljoin(url, href)
                    linked_parsed = urlparse(full_url)
                    linked_domain = linked_parsed.netloc
                    is_external = (linked_domain != parsed_url.netloc)

                    if is_external:
                        if 'error' in full_url.lower() or '404' in full_url or 'notfound' in full_url.lower():
                            error_count += 1
                        elif not linked_domain or linked_domain == '':
                            error_count += 1

                features['ratio_extErrors'] = error_count / len(all_links) if all_links else 0.0

                # external_favicon: ç¶²ç«™æ˜¯å¦ä½¿ç”¨ä¾†è‡ªå¤–éƒ¨åŸŸåçš„ Favicon
                favicon_link = soup.find('link', rel=lambda x: x and 'icon' in x.lower())
                features['external_favicon'] = 0.0
                if favicon_link and 'href' in favicon_link.attrs:
                    favicon_url = urljoin(url, favicon_link['href'])
                    favicon_domain = urlparse(favicon_url).netloc
                    if favicon_domain != parsed_url.netloc:
                        features['external_favicon'] = 1.0

                # links_in_tags: ç‰¹å®š HTML æ¨™ç±¤ä¸­é€£çµçš„æ•¸é‡
                total_links_in_tags = 0
                for tag in soup.find_all(['a', 'script', 'img', 'link', 'iframe', 'form']):
                    if 'href' in tag.attrs:
                        total_links_in_tags += 1
                    if 'src' in tag.attrs:
                        total_links_in_tags += 1
                    if tag.name == 'form' and 'action' in tag.attrs:
                        total_links_in_tags += 1
                features['links_in_tags'] = total_links_in_tags

                # ratio_extMedia: å¤–éƒ¨åª’é«”çš„æ¯”ä¾‹
                media_tags = soup.find_all(['img', 'audio', 'video', 'source'])
                total_media = len(media_tags)
                external_media = 0
                for media_tag in media_tags:
                    src = media_tag.get('src') or media_tag.get('href')
                    if src:
                        media_url = urljoin(url, src)
                        media_domain = urlparse(media_url).netloc
                        if media_domain != parsed_url.netloc:
                            external_media += 1
                features['ratio_extMedia'] = external_media / total_media if total_media > 0 else 0.0

                # safe_anchor: éŒ¨é»é€£çµæ˜¯å¦å®‰å…¨
                features['safe_anchor'] = 1.0
                suspicious_keywords = ['bit.ly', 'tinyurl', 'goo.gl', 't.co', 'fb.me', 'is.gd']

                for link_tag in all_links:
                    href = link_tag['href']
                    if href.startswith('#'):
                        continue
                    full_url = urljoin(url, href)
                    linked_parsed = urlparse(full_url)
                    linked_domain = linked_parsed.netloc
                    is_external = (linked_domain != parsed_url.netloc)

                    if is_external:
                        # æª¢æŸ¥æ˜¯å¦ç‚ºIPåœ°å€
                        try:
                            ipaddress.ip_address(linked_domain)
                            features['safe_anchor'] = 0.0
                            break
                        except ValueError:
                            pass

                        # æª¢æŸ¥å”è­°æ˜¯å¦å®‰å…¨
                        if linked_parsed.scheme not in ['http', 'https', '']:
                            features['safe_anchor'] = 0.0
                            break

                        # æª¢æŸ¥æ˜¯å¦ç‚ºå¯ç–‘çš„çŸ­ç¶²å€æœå‹™
                        if any(keyword in linked_domain.lower() for keyword in suspicious_keywords):
                            features['safe_anchor'] = 0.0
                            break

                # empty_title: ç¶²é æ¨™é¡Œæ˜¯å¦ç‚ºç©º
                features['empty_title'] = 1.0 if not (soup.title and soup.title.string and soup.title.string.strip()) else 0.0

                # domain_in_title: åŸŸåæ˜¯å¦å‡ºç¾åœ¨ç¶²é æ¨™é¡Œä¸­
                features['domain_in_title'] = 0.0
                if soup.title and soup.title.string:
                    if base_domain in soup.title.string.lower():
                        features['domain_in_title'] = 1.0

                # domain_with_copyright: ç¶²ç«™çš„ç‰ˆæ¬Šè³‡è¨Šä¸­æ˜¯å¦åŒ…å«åŸŸå
                features['domain_with_copyright'] = 0.0

                # æª¢æŸ¥ç‰ˆæ¬Šæ–‡æœ¬
                copyright_text = soup.find(text=re.compile(r'Â©|copyright', re.IGNORECASE))
                if copyright_text and base_domain in copyright_text.lower():
                    features['domain_with_copyright'] = 1.0

                # æª¢æŸ¥footerå€åŸŸ
                footer_tags = soup.find_all(['div', 'footer'], class_=re.compile(r'footer|copyright', re.IGNORECASE))
                for footer in footer_tags:
                    if base_domain in footer.get_text().lower():
                        features['domain_with_copyright'] = 1.0
                        break

                # æª¢æŸ¥æ‰€æœ‰åŒ…å«ç‰ˆæ¬Šç›¸é—œæ–‡å­—çš„æ¨™ç±¤
                copyright_tags = soup.find_all(text=re.compile(r'Â©|copyright|all rights reserved', re.IGNORECASE))
                for tag in copyright_tags:
                    if base_domain in tag.lower():
                        features['domain_with_copyright'] = 1.0
                        break

                features['feature_extracted'] = 1.0
            except Exception as e:
                print(f"è§£æ URL '{url}' çš„ HTML æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                features = {col: 0.0 for col in html_feature_columns}
                features['feature_extracted'] = 0.0
        else:
            print(f"æœªèƒ½ç²å– URL '{url}' çš„ HTML å…§å®¹ã€‚æ‰€æœ‰ HTML ç‰¹å¾µå°‡ç‚º 0.0ã€‚")
            features = {col: 0.0 for col in html_feature_columns}
            features['feature_extracted'] = 0.0
        results.append(features)
        time.sleep(0.5)

    features_df = pd.DataFrame(results, index=df.index)
    df_with_features = pd.concat([df, features_df], axis=1)

    return df_with_features

# å®‰å…¨è™•ç† URL çš„å‡½æ•¸
import signal

class TimeoutException(Exception):
    pass

def handler(signum, frame):
    raise TimeoutException()

def safe_process_url(url, max_time=30):
    signal.signal(signal.SIGALRM, handler)
    signal.alarm(max_time)
    try:
        html_content, has_js_redirect = get_html_content(url, timeout=20, max_retries=1)
        signal.alarm(0)
        return html_content, has_js_redirect
    except TimeoutException:
        print(f"â° URL {url} è¶…é {max_time} ç§’ï¼Œå¼·åˆ¶è·³é")
        return None, False
    except Exception as e:
        print(f"âŒ URL {url} è™•ç†å¤±æ•—: {e}")
        return None, False

# ç¯„ä¾‹ä½¿ç”¨
if __name__ == "__main__":
    df = load_data()
    df = df[49500:49800]
    
    # å‘¼å«å‡½å¼å‰µå»ºæ–°ç‰¹å¾µ
    df_with_html_features = extract_html_features(df, url_column='url')
    
    print("\nåŒ…å«æ–° HTML ç‰¹å¾µçš„ DataFrame:")
    print(df_with_html_features)

    print("\næ‰€æœ‰æ–°å‰µå»ºçš„ HTML ç‰¹å¾µæ¬„ä½åŠå…¶å€¼é¡å‹:")
    new_html_cols = [f for f in df_with_html_features.columns if f not in df.columns]
    print({col: df_with_html_features[col].dtype for col in new_html_cols})
```

---

## ğŸ¤– AI ç‰¹å¾µ

### æ¦‚è¿°
ä½¿ç”¨ HTML ç¶²é æ–‡æœ¬æ¨é€çµ¦ GEMINI ç²å–ç›¸é—œæ¬„ä½ï¼Œé€é AI åˆ†æç¶²é å…§å®¹çš„èªæ„ç‰¹å¾µä¾†è­˜åˆ¥é‡£é­šç¶²ç«™ã€‚

### ğŸ“Š ç‰¹å¾µæ¬„ä½åˆ—è¡¨

#### è¡Œç‚ºåˆ†æç‰¹å¾µ
- **creates_urgency**: æ–‡æœ¬æ˜¯å¦å‰µé€ ç·Šè¿«æ„Ÿã€å£“åŠ›æˆ–è¡Œå‹•æœŸé™
- **uses_threats**: æ–‡æœ¬æ˜¯å¦åŒ…å«å¨è„…ã€å¸³æˆ¶æš«åœè­¦å‘Šæˆ–å…¶ä»–è² é¢å¾Œæœ
- **requests_sensitive_info**: æ–‡æœ¬æ˜¯å¦æ˜ç¢ºæˆ–éš±å«åœ°è¦æ±‚ç™»å…¥æ†‘è­‰ã€è²¡å‹™è©³æƒ…æˆ–å€‹äººèº«ä»½è³‡è¨Š
- **offers_unrealistic_rewards**: æ–‡æœ¬æ˜¯å¦æä¾›çœ‹ä¼¼ä¸åˆ‡å¯¦éš›çš„çå‹µã€çå“æˆ–å½©ç¥¨ä¸­ç

#### å“è³ªåˆ†æç‰¹å¾µ
- **has_spelling_grammar_errors**: æ˜¯å¦å­˜åœ¨æ˜é¡¯çš„æ‹¼å¯«æˆ–èªæ³•éŒ¯èª¤ï¼Œä¸ç¬¦åˆå°ˆæ¥­ç¶²ç«™ç‰¹å¾µ
- **language_professionalism_score**: æ–‡æœ¬çš„å°ˆæ¥­æ€§å’Œèªæ³•æ­£ç¢ºæ€§è©•åˆ† (0-10åˆ†)

#### èº«ä»½åˆ†æç‰¹å¾µ
- **impersonated_brand**: æ–‡æœ¬è©¦åœ–å†’å……çš„ç‰¹å®šå“ç‰Œæˆ–å…¬å¸åç¨±
- **overall_phishing_likelihood_score**: åŸºæ–¼æ‰€æœ‰æ–‡æœ¬ç·šç´¢çš„æ•´é«”é‡£é­šå¯èƒ½æ€§è©•åˆ† (0-10åˆ†)
- **summary_of_intent**: åŸºæ–¼æ–‡æœ¬å…§å®¹çš„é é¢æ„åœ–ç°¡è¦ç¸½çµ

#### ç‹€æ…‹ç‰¹å¾µ
- **fetch_status**: ç¶²é å…§å®¹çˆ¬å–ç‹€æ…‹
- **gemini_status**: Gemini API åˆ†æç‹€æ…‹

### ğŸ› ï¸ ç’°å¢ƒè¨­å®š

```bash
# å®‰è£å¿…è¦å¥—ä»¶
pip install google-generativeai
pip install selenium
pip install webdriver-manager
apt-get update
apt install chromium-chromedriver

# æ‰‹å‹•å®‰è£èˆ‡æœ€æ–° WebDriver ç›¸ç¬¦çš„ Chrome ç€è¦½å™¨
wget -q https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/114.0.5735.90/linux64/chrome-linux64.zip
unzip -q chrome-linux64.zip -d /bin/
```

### ğŸ’» ç¨‹å¼ç¢¼å¯¦ä½œ

```python
import pandas as pd
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
import os
import time
import json
import numpy as np
from typing import Optional, Dict, Any, Tuple
from google.colab import userdata
from google.colab import files
import time
from google.api_core import exceptions
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# --- å…¨åŸŸå¸¸æ•¸è¨­å®š ---
MODEL_NAME = 'gemini-2.5-flash'
DELAY_SECONDS = 6
initial_data = None

# --- æª”æ¡ˆè·¯å¾‘è¨­å®š ---
ORIGINAL_DATA_FILE = 'phishing_dataset_expansion_1.csv'
IN_PROGRESS_FILE = 'phishing_dataset_expansion_1_Gemini.csv'

def setup_gemini() -> Optional[genai.GenerativeModel]:
    """
    è¨­å®š Google API é‡‘é‘°ä¸¦åˆå§‹åŒ– Gemini æ¨¡å‹ã€‚
    Returns:
        æˆåŠŸæ™‚è¿”å›åˆå§‹åŒ–çš„æ¨¡å‹ç‰©ä»¶ï¼Œå¤±æ•—æ™‚è¿”å› Noneã€‚
    """
    print("æ­£åœ¨è¨­å®š Gemini API...")
    try:
        # å¾ Colab çš„å¯†é‘°ç®¡ç†å™¨ä¸­è®€å– API é‡‘é‘°
        api_key = userdata.get('GOOGLE_API_KEY')
        genai.configure(api_key=api_key)
        generation_config=genai.GenerationConfig(
            temperature=0.4,
            response_mime_type="application/json"
        )
        model = genai.GenerativeModel(
            MODEL_NAME,
            generation_config=generation_config)
        print("âœ… Gemini API è¨­å®šæˆåŠŸï¼")
        return model
    except userdata.SecretNotFoundError:
        print("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åç‚º 'GOOGLE_API_KEY' çš„å¯†é‘°ã€‚è«‹æª¢æŸ¥å·¦å´ã€Œé‘°åŒ™ã€åœ–ç¤ºä¸­çš„è¨­å®šã€‚")
        exit()
    except Exception as e:
        print(f"è¨­å®šæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")
        exit()

def fetch_webpage_text(url: str) -> Tuple[Optional[str], str]:
    """
    çˆ¬å–æŒ‡å®š URL çš„ç´”æ–‡å­—å…§å®¹ï¼Œå„ªå…ˆä½¿ç”¨éœæ…‹çˆ¬å–ï¼Œè‹¥å…§å®¹ç‚ºç©ºå‰‡åˆ‡æ›è‡³å‹•æ…‹çˆ¬å–ã€‚

    Args:
        url: è¦çˆ¬å–çš„ç¶²ç«™ URLã€‚

    Returns:
        tuple: (ç¶²é ç´”æ–‡å­—, ç‹€æ…‹ç¢¼)ã€‚
            - æˆåŠŸæ™‚è¿”å› (text, 'OK')
            - éœæ…‹çˆ¬å–ä½†å…§å®¹ç‚ºç©ºæ™‚è¿”å› (None, 'OK_Empty')
            - å‹•æ…‹çˆ¬å–æˆåŠŸæ™‚è¿”å› (text, 'OK_Dynamic')
            - å¤±æ•—æ™‚è¿”å› (None, 'Error_XXX')
    """
    # å˜—è©¦éœæ…‹çˆ¬å–
    text, status = _fetch_static_content(url)
    
    # å¦‚æœéœæ…‹çˆ¬å–çµæœç‚ºç©ºï¼Œå‰‡å˜—è©¦å‹•æ…‹çˆ¬å–
    if status == 'OK_Empty':
        print(f"ç¶²é  {url} éœæ…‹å…§å®¹ç‚ºç©ºï¼Œåˆ‡æ›è‡³å‹•æ…‹çˆ¬å–...")
        return _fetch_dynamic_content(url)

    return text, status

def _fetch_static_content(url: str) -> Tuple[Optional[str], str]:
    """
    ä½¿ç”¨ requests çˆ¬å–éœæ…‹ç¶²é å…§å®¹ã€‚
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
            'Accept-Language': 'zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.google.com/',
            'Connection': 'keep-alive'
        }
        response = requests.get(url, headers=headers, allow_redirects=False, timeout=10)

        # æª¢æŸ¥ç‹€æ…‹ç¢¼ï¼Œè™•ç†é‡æ–°å°å‘
        if response.status_code == 301 or response.status_code == 302:
            print(f"ç¶²é  {url} ç™¼ç”Ÿé‡æ–°å°å‘ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
            return None, f'Redirect_{response.status_code}'
        
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        web_text = soup.get_text(separator=' ', strip=True)

        print(f"ç¶²é  {url} éœæ…‹çˆ¬å–å®Œæˆ...")
        return (web_text, 'OK') if web_text else (None, 'OK_Empty')

    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            return None, 'Error_404'
        else:
            return None, f'Error_HTTP_{e.response.status_code}'

    except requests.exceptions.RequestException as e:
        print(f"    âŒ éŒ¯èª¤: ç„¡æ³•çˆ¬å–ç¶²é  {url}ã€‚éŒ¯èª¤è¨Šæ¯: {e}")
        return None, 'Error_Request'
    except Exception as e:
        print(f"    âŒ éŒ¯èª¤: çˆ¬å–ç¶²é æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ã€‚éŒ¯èª¤è¨Šæ¯: {e}")
        return None, 'Error_Unknown'

def _fetch_dynamic_content(url: str) -> Tuple[Optional[str], str]:
    """
    ä½¿ç”¨ Selenium çˆ¬å–å‹•æ…‹ç¶²é å…§å®¹ã€‚
    """
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-setuid-sandbox')
        chrome_options.add_argument('--user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"')
        chrome_options.binary_location = "/bin/chrome-linux64/chrome"
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)

        driver.get(url)
        time.sleep(10)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        web_text = soup.get_text(separator=' ', strip=True)

        print(f"ç¶²é  {url} å‹•æ…‹çˆ¬å–å®Œæˆ...")
        return (web_text, 'OK_Dynamic') if web_text else (None, 'OK_Dynamic_Empty')
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: ä½¿ç”¨ Selenium çˆ¬å–ç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚è¨Šæ¯: {e}")
        return None, 'Error_Selenium'
    finally:
        if 'driver' in locals():
            driver.quit()

def analyze_text_with_gemini(model: genai.GenerativeModel, web_text: str) -> Optional[Dict[str, Any]]:
    """
    ä½¿ç”¨ Gemini åˆ†ææ–‡æœ¬ä¸¦å›å‚³çµæ§‹åŒ–çš„ JSON ç‰¹å¾µã€‚
    Args:
        model: å·²åˆå§‹åŒ–çš„ Gemini æ¨¡å‹ç‰©ä»¶ã€‚
        web_text: å¾ç¶²é çˆ¬å–ä¸‹ä¾†çš„ç´”æ–‡å­—ã€‚
    Returns:
        æˆåŠŸæ™‚è¿”å›åŒ…å«èªæ„ç‰¹å¾µçš„å­—å…¸ï¼Œå¤±æ•—æ™‚è¿”å› Noneã€‚
    """
    prompt_template = f"""
    # Role: Cyber Security Analyst

    You are a senior cybersecurity analyst specializing in phishing detection. Your task is to objectively analyze the provided web page text and determine if it exhibits characteristics of a phishing attempt, strictly based on the content provided.

    Based *only* on the text provided, generate a JSON object containing the following features. You must remain objective and provide assessments based solely on observable evidence within the text.

    - `creates_urgency`: (boolean) Does the text create a sense of urgency, pressure, or a deadline for the user to act quickly?
    - `uses_threats`: (boolean) Does the text contain threats, warnings of account suspension, or other negative consequences if the user doesn't act?
    - `requests_sensitive_info`: (boolean) Does the text explicitly or implicitly ask for login credentials (username, password), financial details (credit card numbers), or personal identification?
    - `offers_unrealistic_rewards`: (boolean) Does the text offer prizes, lottery winnings, or other rewards that seem too good to be true?
    - `has_spelling_grammar_errors`: (boolean) Are there noticeable spelling or grammatical errors that are uncharacteristic of a professional website?
    - `impersonated_brand`: (string) What specific brand or company (e.g., "Facebook", "Google", "ä¸­è¯éƒµæ”¿") is the text trying to impersonate? This judgment must be based *solely* on mentions, logos, or design cues implied by the *text description* provided. If no specific brand is clear from the text, return "N/A".
    - `language_professionalism_score`: (numeric, 0-10) Rate the professionalism and grammatical correctness of the text.
        - 0-2: Severely unprofessional, numerous glaring spelling/grammar errors, unclear language.
        - 3-5: Noticeable errors, somewhat unprofessional, but understandable.
        - 6-8: Mostly professional, minor or infrequent errors.
        - 9-10: Highly professional, no discernible errors, official tone.
        - Only assign scores of 0 or 10 if there is overwhelming, undeniable evidence to support such an extreme assessment. Most cases will fall between 2 and 8.
    - `overall_phishing_likelihood_score`: (numeric, 0-10) Based on ALL textual clues (urgency, threats, info requests, rewards, language errors, impersonation), what is the overall likelihood that this is a phishing page?
        - 0-2: Very low likelihood; text appears legitimate or benign.
        - 3-5: Moderate likelihood; some suspicious elements, but not conclusive.
        - 6-8: High likelihood; strong indicators of phishing.
        - 9-10: Extremely high likelihood; undeniable phishing attempt based on text.
        - Assign scores of 0 or 10 only if the evidence for or against phishing is absolute and unequivocal. Favor scores between 2 and 8 for nuanced cases.
    - `summary_of_intent`: (string) A brief, one-sentence summary explaining the likely intent of the page based on its text.

    ---
    # Web Page Text to Analyze:

    {web_text}

    ---
    IMPORTANT INTERNAL CHECK: Before generating the JSON, review your assessment. Ensure each boolean and score is based *strictly* on the provided `web_text` and adheres to the specified scoring criteria and objectivity requirements. If any part of your answer does not meet these standards, re-evaluate and correct it before outputting. Do not include this check in your final JSON output.

    # JSON Output:
    """

    # è‡ªå‹•é‡è©¦æ©Ÿåˆ¶
    max_retries = 3
    base_wait_time = 5

    for i in range(max_retries):
        try:
            response = model.generate_content(prompt_template)
            json_string = response.text.strip().replace('```json', '').replace('```', '')
            return json.loads(json_string), 'OK'
        except exceptions.ResourceExhausted:
            print("    âŒ éŒ¯èª¤: æ”¶åˆ° 429 é »ç‡é™åˆ¶éŒ¯èª¤ã€‚æ”¾æ£„æ­¤ç­†è³‡æ–™çš„ Gemini åˆ†æã€‚")
            return None, 'Error_429_RateLimit'
        except exceptions.ServiceUnavailable as e:
            wait_time = base_wait_time * (2 ** i)
            print(f"    âš ï¸ è­¦å‘Š: æ”¶åˆ° 503 ä¼ºæœå™¨ä¸å¯ç”¨éŒ¯èª¤ã€‚å°‡åœ¨ {wait_time} ç§’å¾Œé€²è¡Œç¬¬ {i+1}/{max_retries} æ¬¡é‡è©¦...")
            time.sleep(wait_time)
        except Exception as e:
            print(f"    âŒ éŒ¯èª¤: Gemini API åˆ†ææˆ– JSON è§£æå¤±æ•—ã€‚éŒ¯èª¤è¨Šæ¯: {e}")
            return None, 'Error_JSON_Parse'

    print(f"    âŒ éŒ¯èª¤: é‡è©¦ {max_retries} æ¬¡å¾Œä»ç„¶å¤±æ•— (503)ã€‚")
    return None, 'Error_503_Retries_Failed'

def process_dataframe(df_batch: pd.DataFrame, model: genai.GenerativeModel) -> pd.DataFrame:
    """
    éæ­· DataFrameï¼Œå°æ¯å€‹ URL é€²è¡Œåˆ†æä¸¦æ“´å……ç‰¹å¾µã€‚
    Args:
        df: åŸå§‹çš„ DataFrameã€‚
        model: å·²åˆå§‹åŒ–çš„ Gemini æ¨¡å‹ç‰©ä»¶ã€‚
    Returns:
        æ“´å……äº†èªæ„ç‰¹å¾µå¾Œçš„ DataFrameã€‚
    """
    print("\nğŸš€ é–‹å§‹è™•ç† DataFrame ä¸­å°æ‰¹æ¬¡çš„ URL...\n")
    df_processed = df_batch.copy()

    for index, row in df_processed.iterrows():
        print(f"--- æ­£åœ¨è™•ç†ç¬¬ {index + 1}/{len(df_processed)} ç­†è³‡æ–™: {row['url']} ---")

        web_text, fetch_status = safe_process_url(row['url'])

        df_processed.loc[index, 'fetch_status'] = fetch_status

        if not web_text:
            df_processed.loc[index, 'gemini_status'] = 'Skipped_Due_To_Fetch_Error'
            print("    ç¶²é è¢«é‡æ–°å°å‘æˆ–æ˜¯æ–‡æœ¬ç‚ºç©ºã€‚")
            print("    â­ï¸ è·³éæ­¤ç­†è³‡æ–™ã€‚")
            continue

        semantic_features, gemini_status = analyze_text_with_gemini(model, web_text)
        df_processed.loc[index, 'gemini_status'] = gemini_status
        if not semantic_features:
            print("    â­ï¸ è·³éæ­¤ç­†è³‡æ–™ã€‚")
            continue

        print("    âœ… æˆåŠŸå–å¾—ç‰¹å¾µï¼Œæ­£åœ¨æ›´æ–° DataFrame...")
        for key, value in semantic_features.items():
            if key in df_processed.columns:
                df_processed.loc[index, key] = value

        print(f"    â° è™•ç†å®Œæˆï¼Œç­‰å¾… {DELAY_SECONDS} ç§’...\n")
        time.sleep(DELAY_SECONDS)

    return df_processed

def load_data() -> pd.DataFrame:
    uploaded = files.upload()

    for fn in uploaded.keys():
        print('User uploaded file "{name}" with length {length} bytes'.format(
            name=fn, length=len(uploaded[fn])))

    file_name = next(iter(uploaded))
    df = pd.read_csv(file_name)

    print("âœ… è³‡æ–™é›†è¼‰å…¥æˆåŠŸï¼")
    return df

# å®‰å…¨è™•ç† URL çš„å‡½æ•¸
import signal

class TimeoutException(Exception):
    pass

def handler(signum, frame):
    raise TimeoutException()

def safe_process_url(url, max_time=90):
    signal.signal(signal.SIGALRM, handler)
    signal.alarm(max_time)
    try:
        web_text, fetch_status = fetch_webpage_text(url)
        signal.alarm(0)
        return web_text, fetch_status
    except TimeoutException:
        print(f"â° URL {url} è¶…é {max_time} ç§’ï¼Œå¼·åˆ¶è·³é")
        return None, 'Error_Connect'
    except Exception as e:
        print(f"âŒ URL {url} è™•ç†å¤±æ•—: {e}")
        return None, 'Error_Unknown'

def main():
    global initial_data
    """
    ä¸»å‡½å¼ï¼Œè² è²¬ä¸²é€£æ•´å€‹è™•ç†æµç¨‹ã€‚
    """
    model = setup_gemini()
    if not model:
        return

    BATCH_SIZE = 500

    # è¼‰å…¥é€²åº¦
    df = load_data()
    initial_data = df
    if os.path.exists(IN_PROGRESS_FILE):
        print(f"ç™¼ç¾è™•ç†ä¸­æª”æ¡ˆï¼Œå¾ '{IN_PROGRESS_FILE}' è¼‰å…¥é€²åº¦...")
        df = pd.read_csv(IN_PROGRESS_FILE)
    else:
        print(f"æœªç™¼ç¾è™•ç†ä¸­æª”æ¡ˆï¼Œå¾åŸå§‹æª” '{ORIGINAL_DATA_FILE}' é–‹å§‹æ–°ä»»å‹™...")
        try:
            # é å…ˆå»ºç«‹æ–°æ¬„ä½ä¸¦å¡«æ»¿ NaN
            new_columns = [
                'gemini_status',
                'fetch_status',
                'creates_urgency', 'uses_threats', 'requests_sensitive_info',
                'offers_unrealistic_rewards', 'has_spelling_grammar_errors',
                'impersonated_brand', 'language_professionalism_score',
                'overall_phishing_likelihood_score', 'summary_of_intent'
            ]
            for col in new_columns:
                df[col] = np.nan
        except FileNotFoundError:
            print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°åŸå§‹è³‡æ–™æª” '{ORIGINAL_DATA_FILE}'ã€‚è«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å­˜åœ¨ã€‚")
            return

    # å°‹æ‰¾æœªå®Œæˆçš„ä»»å‹™
    unprocessed_mask = df['fetch_status'].isnull()
    unprocessed_df = df[unprocessed_mask]
    
    if unprocessed_df.empty:
        print("\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼æ‰€æœ‰è³‡æ–™éƒ½å·²è™•ç†å®Œç•¢ï¼ğŸ‰ğŸ‰ğŸ‰")
        return

    # é¸å–æœ¬æ¬¡æ‰¹æ¬¡
    batch_to_process = unprocessed_df.head(BATCH_SIZE)

    # è™•ç†æ‰¹æ¬¡
    processed_batch = process_dataframe(batch_to_process, model)

    # æ›´æ–°èˆ‡å„²å­˜
    df.update(processed_batch)

    print("="*50)
    print("\nâœ… æ‰€æœ‰è³‡æ–™è™•ç†å®Œç•¢ï¼")
    print("\n--- æœ€çµ‚æ“´å……å¾Œçš„ DataFrame ---")

    display_columns = ['url', 'target'] + [col for col in df.columns if col not in initial_data]
    print(df[display_columns])

    # å°‡æœ€çµ‚çµæœå„²å­˜åˆ°æª”æ¡ˆ
    try:
        df.to_csv(IN_PROGRESS_FILE, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ é€²åº¦å·²æ›´æ–°ä¸¦å„²å­˜è‡³ '{IN_PROGRESS_FILE}'")
        remaining = len(df[df['overall_phishing_likelihood_score'].isnull()])
        print(f"ğŸŸ¢ ç›®å‰å‰©é¤˜ {remaining} ç­†è³‡æ–™å¾…è™•ç†ã€‚")
        num = df[(df['fetch_status'].notna())].shape[0]
        count = df[(df['fetch_status'].notna()) & (df['gemini_status'] != 'OK')].shape[0]
        print(f'"çˆ¬å–éçš„{num}ç­†è³‡æ–™ä¸­ï¼Œ{count}ç­†è³‡æ–™ç„¡æ³•çˆ¬å–åˆ°ç¶²é å…§å®¹ã€‚')
    except Exception as e:
        print(f"\nâŒ å„²å­˜æª”æ¡ˆå¤±æ•—: {e}")

if __name__ == "__main__":
    main()
```

---

## ğŸ“ ç¸½çµ

æœ¬æ–‡ä»¶ä»‹ç´¹äº†ä¸‰ç¨®ä¸åŒé¡å‹çš„ç‰¹å¾µæå–æ–¹æ³•ï¼š

### ğŸ”— URL ç‰¹å¾µ (45å€‹ç‰¹å¾µ)
- **åŸºæœ¬é•·åº¦ç‰¹å¾µ**: URL å’Œ hostname é•·åº¦
- **å­—ç¬¦çµ±è¨ˆç‰¹å¾µ**: å„ç¨®ç¬¦è™Ÿçš„æ•¸é‡çµ±è¨ˆ
- **åŸŸåç‰¹å¾µ**: åŸŸåçµæ§‹å’Œå”è­°ç›¸é—œç‰¹å¾µ
- **æ¯”ä¾‹ç‰¹å¾µ**: æ•¸å­—å­—ç¬¦æ¯”ä¾‹
- **æ–‡å­—ç‰¹å¾µ**: å–®è©é•·åº¦çµ±è¨ˆ

### ğŸŒ HTML ç‰¹å¾µ (17å€‹ç‰¹å¾µ)
- **è½‰å‘æª¢æ¸¬**: Meta å’Œ JavaScript è½‰å‘
- **å…§å®¹åˆ†æ**: é‡£é­šæç¤ºè©ã€å“ç‰Œä¸€è‡´æ€§
- **é€£çµåˆ†æ**: å…§å¤–éƒ¨é€£çµæ¯”ä¾‹å’Œå®‰å…¨æ€§
- **è³‡æºåˆ†æ**: å¤–éƒ¨è³‡æºä½¿ç”¨æƒ…æ³
- **ç‹€æ…‹ç‰¹å¾µ**: ç‰¹å¾µæå–æˆåŠŸæ¨™è¨˜

### ğŸ¤– AI ç‰¹å¾µ (11å€‹ç‰¹å¾µ)
- **è¡Œç‚ºåˆ†æ**: ç·Šè¿«æ„Ÿã€å¨è„…ã€æ•æ„Ÿè³‡è¨Šè«‹æ±‚
- **å“è³ªåˆ†æ**: èªè¨€å°ˆæ¥­æ€§è©•åˆ†
- **èº«ä»½åˆ†æ**: å“ç‰Œå†’å……å’Œæ•´é«”é‡£é­šå¯èƒ½æ€§
- **ç‹€æ…‹ç‰¹å¾µ**: çˆ¬å–å’Œåˆ†æç‹€æ…‹è¿½è¹¤

æ¯ç¨®ç‰¹å¾µéƒ½æœ‰å®Œæ•´çš„ç¨‹å¼ç¢¼å¯¦ä½œï¼ŒåŒ…å«éŒ¯èª¤è™•ç†ã€é‡è©¦æ©Ÿåˆ¶å’Œé€²åº¦ä¿å­˜åŠŸèƒ½ï¼Œé©åˆç”¨æ–¼å¤§è¦æ¨¡è³‡æ–™é›†çš„é‡£é­šç¶²ç«™æª¢æ¸¬ä»»å‹™ã€‚


