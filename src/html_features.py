import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import re
import time
from collections import Counter
import ipaddress
import signal
from tqdm import tqdm
from typing import Tuple, Optional
# ====== å¯é¸: å¦‚æœéœ€è¦è™•ç†å‹•æ…‹å…§å®¹ï¼Œè«‹å–æ¶ˆè¨»é‡‹ä»¥ä¸‹å…§å®¹ä¸¦å®‰è£ç›¸é—œåº« ======
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
# =====================================================================


def get_html_content(url, timeout=20, max_retries=2):
    """
    æ ¹æ“š URL ç²å–ç¶²é çš„å®Œæ•´ HTML å…§å®¹ï¼Œä¸¦åµæ¸¬æ˜¯å¦æœ‰è½‰å‘ã€‚
    å…ˆä½¿ç”¨ requests çˆ¬å–ï¼Œå¦‚æœå…§å®¹ç‚ºç©ºæˆ–å¤±æ•—å‰‡ä½¿ç”¨ Seleniumã€‚
    Args:
        url (str): ç›®æ¨™ç¶²é çš„ URLã€‚
        timeout (int): è«‹æ±‚è¶…æ™‚æ™‚é–“ (ç§’)ã€‚
        max_retries (int): å¤±æ•—æ™‚é‡è©¦æ¬¡æ•¸ã€‚
    Returns:
        tuple[str, bool]: ç¶²é çš„ HTML å…§å®¹ï¼Œå¦‚æœç²å–å¤±æ•—å‰‡è¿”å› Noneã€‚
                           ä»¥åŠä¸€å€‹å¸ƒæ—å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ç™¼ç”Ÿäº†è½‰å‘ã€‚
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.7258.66 Safari/537.36'
    }

    # ç¬¬ä¸€æ­¥ï¼šå˜—è©¦ä½¿ç”¨ requests çˆ¬å–
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser')
            body_content = soup.find('body')
            # æª¢æŸ¥å…§å®¹æ˜¯å¦ç‚ºç©ºæˆ–éçŸ­ï¼ˆå¯èƒ½æ˜¯å‹•æ…‹å…§å®¹ï¼‰
            if html_content and len(html_content.strip()) > 100:
                # æª¢æŸ¥é é¢å…§å®¹æ˜¯å¦åŒ…å« "page not found" ç›¸é—œé—œéµè©
                not_found_keywords = ['page not found', 'error 404', 'page does not exist', 'æ‰¾ä¸åˆ°é é¢', 'é é¢ä¸å­˜åœ¨']
                matched_keywords = 0
                try:
                  body_text = body_content.get_text(strip=True)
                  body_lower = body_text.lower()

                  # è¨ˆç®—åŒ¹é…çš„é—œéµè©æ•¸é‡
                  matched_keywords = sum(1 for keyword in not_found_keywords if keyword in body_lower)
                except:
                  html_lower = html_content.lower()

                  # è¨ˆç®—åŒ¹é…çš„é—œéµè©æ•¸é‡
                  matched_keywords = sum(1 for keyword in not_found_keywords if keyword in html_lower)
                if matched_keywords >= 2:
                    print(f"âŒ URL {url} é é¢å…§å®¹åŒ…å« {matched_keywords} å€‹ 'page not found' ç›¸é—œé—œéµè©ï¼Œç›´æ¥è·³é")
                    return None, False

                print(f"âœ… ä½¿ç”¨ requests æˆåŠŸç²å– {url} çš„å…§å®¹")
                return html_content, False
            else:
                print(f"âš ï¸  requests ç²å–çš„å…§å®¹ç‚ºç©ºæˆ–éçŸ­ï¼Œå°‡å˜—è©¦ä½¿ç”¨ Selenium")
                break

        except requests.exceptions.HTTPError as e:
            # æª¢æŸ¥æ˜¯å¦ç‚º 404 æˆ–å…¶ä»– "page not found" ç›¸é—œéŒ¯èª¤
            if response.status_code == 404:
                print(f"âŒ URL {url} è¿”å› 404 éŒ¯èª¤ï¼Œç›´æ¥è·³é")
                return None, False
            elif response.status_code >= 400:
                print(f"âŒ URL {url} è¿”å› HTTP {response.status_code} éŒ¯èª¤ï¼Œç›´æ¥è·³é")
                return None, False
            else:
                print(f"[é‡è©¦ {attempt+1}/{max_retries}] Requests ç²å– {url} æ™‚ç™¼ç”Ÿ HTTP éŒ¯èª¤: {e}")
                if attempt == max_retries:
                    print(f"âŒ Requests é‡è©¦ {max_retries} æ¬¡å¾Œä»ç„¶å¤±æ•—ï¼Œå°‡å˜—è©¦ä½¿ç”¨ Selenium")
                    break
                time.sleep(0.5)
        except requests.exceptions.RequestException as e:
            print(f"[é‡è©¦ {attempt+1}/{max_retries}] Requests ç²å– {url} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            if attempt == max_retries:
                print(f"âŒ Requests é‡è©¦ {max_retries} æ¬¡å¾Œä»ç„¶å¤±æ•—ï¼Œå°‡å˜—è©¦ä½¿ç”¨ Selenium")
                break
            time.sleep(1)

    # ç¬¬äºŒæ­¥ï¼šå¦‚æœ requests å¤±æ•—æˆ–å…§å®¹ç‚ºç©ºï¼Œä½¿ç”¨ Selenium
    print(f"ğŸ”„ é–‹å§‹ä½¿ç”¨ Selenium çˆ¬å– {url},æš«æ™‚è¨­ç‚ºå¤±æ•—")
    return None, False
    # return _fetch_dynamic_content(url)

def _fetch_dynamic_content(url: str) -> Tuple[Optional[str], str]:
    """
    ä½¿ç”¨ Selenium çˆ¬å–å‹•æ…‹ç¶²é å…§å®¹ã€‚
    """
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless') # å•Ÿç”¨ç„¡é ­æ¨¡å¼ï¼Œè®“ç€è¦½å™¨åœ¨èƒŒæ™¯é‹è¡Œï¼Œä¸é¡¯ç¤ºè¦–çª—
        chrome_options.add_argument('--no-sandbox') # é¿å…åœ¨ Colab ç’°å¢ƒä¸‹å¯èƒ½ç™¼ç”Ÿçš„æ¬Šé™å•é¡Œ
        chrome_options.add_argument('--disable-dev-shm-usage') # è§£æ±º /dev/shm åˆ†å€ç©ºé–“ä¸è¶³çš„å•é¡Œï¼Œé€™åœ¨ Docker æˆ– Colab ä¸­å¾ˆå¸¸è¦‹
        chrome_options.add_argument('--disable-gpu') # ç¦ç”¨ GPU åŠ é€Ÿ
        chrome_options.add_argument('--disable-setuid-sandbox') # è®“ Chrome å¯ä»¥åœ¨ä¸å®‰å…¨çš„ç’°å¢ƒä¸‹åŸ·è¡Œ

        # æ¨¡æ“¬ä¸€å€‹çœŸå¯¦çš„ User-Agentï¼Œé¿å…è¢«ç¶²ç«™åµæ¸¬ç‚ºçˆ¬èŸ²
        chrome_options.add_argument('--user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"')
        # æŒ‡å®š Chrome åŸ·è¡Œæª”è·¯å¾‘ï¼ŒæŒ‡å‘æˆ‘å€‘æ‰‹å‹•å®‰è£çš„ä½ç½®
        chrome_options.binary_location = "/bin/chrome-linux64/chrome"
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)

        driver.get(url)
        # ç­‰å¾…ç¶²é å…§å®¹è¼‰å…¥ï¼Œå¯ä¾æ“šç¶²ç«™ç‰¹æ€§èª¿æ•´æ™‚é–“
        time.sleep(10)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        web_text = soup.get_text(separator=' ', strip=True)

        print(f"ç¶²é  {url} å‹•æ…‹çˆ¬å–å®Œæˆ...")
        return (web_text, 'OK_Dynamic') if web_text else (None, 'OK_Dynamic_Empty')
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: ä½¿ç”¨ Selenium çˆ¬å–ç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚è¨Šæ¯: {e}")
        return None, 'Error_Selenium'
    finally:
        if 'driver' in locals():
            driver.quit()

def extract_html_features(df: pd.DataFrame, url_column: str = 'url') -> pd.DataFrame:
    """
    åœ¨ DataFrame ä¸­ç‚ºæŒ‡å®šçš„ URL æ¬„ä½çˆ¬å– HTML å…§å®¹ä¸¦å‰µå»ºå¤šå€‹ç‰¹å¾µã€‚
    æœƒè‡ªå‹•å…ˆä½¿ç”¨ requests çˆ¬å–ï¼Œå¦‚æœå…§å®¹ç‚ºç©ºå‰‡ä½¿ç”¨ Seleniumã€‚

    Args:
        df (pd.DataFrame): åŒ…å« URL æ¬„ä½çš„ DataFrameã€‚
        url_column (str): DataFrame ä¸­åŒ…å« URL çš„æ¬„ä½åç¨±ã€‚é è¨­ç‚º 'url'ã€‚

    Returns:
        pd.DataFrame: åŒ…å«æ–°å‰µå»ºç‰¹å¾µæ¬„ä½çš„ DataFrameã€‚
    """

    if url_column not in df.columns:
        raise ValueError(f"DataFrame ä¸­æœªæ‰¾åˆ°æŒ‡å®šçš„ URL æ¬„ä½: '{url_column}'")

    # ç¢ºä¿æ‰€æœ‰ URL éƒ½æ˜¯å­—ä¸²é¡å‹ï¼Œä¸¦è™•ç†å¯èƒ½çš„ NaN
    df[url_column] = df[url_column].astype(str).replace('nan', '')

    # æ›´æ–°ç‰¹å¾µæ¬„ä½åˆ—è¡¨ï¼ŒåŠ å…¥å…©å€‹è½‰å‘ç‰¹å¾µ
    html_feature_columns = [
        'phish_hints', 'domain_in_brand', 'nb_hyperlinks', 'ratio_intHyperlinks',
        'ratio_extHyperlinks', 'ratio_extRedirection', 'ratio_extErrors',
        'external_favicon', 'links_in_tags', 'ratio_extMedia', 'safe_anchor',
        'empty_title', 'domain_in_title', 'domain_with_copyright',
        'has_meta_refresh', 'has_js_redirect',
        'feature_extracted'  # æ–°å¢æ¬„ä½
    ]

    results = []

    for index, row in df.iterrows():
        url = row[url_column]

        features = {col: 0.0 for col in html_feature_columns}

        if not url:
            features['feature_extracted'] = 0.0
            results.append(features)
            continue

        print(f"æ­£åœ¨è™•ç†ç¬¬ {index+1} / {len(df)} ç­† URL: {url}")

        # è¨­å®šå–®å€‹ URL çš„æœ€å¤§è™•ç†æ™‚é–“ï¼ˆç§’ï¼‰
        max_url_time = 30

        try:
            # --- é—œéµä¿®æ”¹ï¼šå‘¼å« get_html_content ä¸¦æ¥æ”¶å…©å€‹è¿”å›å€¼ ---
            html_content, has_js_redirect = safe_process_url(url, max_time=120)
        except Exception as e:
            print(f"âŒ è™•ç† URL {url} æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            features['feature_extracted'] = 0.0
            results.append(features)
            continue

        if html_content:
            try:
                soup = BeautifulSoup(html_content, 'html.parser')
                parsed_url = urlparse(url)
                base_domain = parsed_url.netloc.split(':')[0]
                if base_domain.startswith('www.'):
                    base_domain = base_domain[4:]
                redirect_keywords = ["window.location.href"]
                # --- æ–°å¢çš„åµæ¸¬é‚è¼¯ ---
                # 15. åµæ¸¬ meta è½‰å‘
                meta_refresh_tag = soup.find('meta', attrs={'http-equiv': lambda x: x and x.lower() == 'refresh'})
                if meta_refresh_tag:
                  content = meta_refresh_tag.get("content", "")
                  features['has_meta_refresh'] = 1.0 if "url=" in content.lower() else 0.0  # ç¢ºèªæ˜¯ redirectï¼Œä¸æ˜¯å–®ç´” reload
                else:
                  features['has_meta_refresh'] = 0.0

                # 16. åµæ¸¬ JavaScript è½‰å‘ (ä¾†è‡ª get_html_content çš„è¿”å›å€¼)
                features['has_js_redirect'] = 1.0 if soup.find("script", string=lambda s: any(k in s for k in redirect_keywords) if s else False) else 0.0
                # ----------------------

                # --- 1. phish_hints: HTML å…§å®¹ä¸­æ˜¯å¦å­˜åœ¨å¸¸è¦‹çš„é‡£é­šæç¤ºè©èª ---
                phish_keywords = ['login', 'signin', 'account update', 'verify account',
                                  'security alert', 'password', 'bank', 'paypal', 'credit card',
                                  'ç·Šæ€¥', 'é©—è­‰', 'ç™»å…¥', 'å¸³æˆ¶æ›´æ–°', 'å®‰å…¨è­¦å‘Š', 'å¯†ç¢¼']
                text_content = soup.get_text().lower()
                features['phish_hints'] = 1 if any(kw in text_content for kw in phish_keywords) else 0.0

                # --- 2. domain_in_brand: ç¶²ç«™å…§å®¹ä¸­æåŠçš„å“ç‰Œåç¨±æ˜¯å¦èˆ‡åŸŸåä¸€è‡´ ---
                # ç°¡åŒ–è™•ç†: æª¢æŸ¥åŸŸåæ ¸å¿ƒéƒ¨åˆ†æ˜¯å¦å‡ºç¾åœ¨ meta description, title æˆ– copyright ä¸­
                # æ›´ç²¾ç¢ºéœ€è¦å“ç‰Œåç¨±åˆ—è¡¨æˆ– NLP å¯¦é«”è­˜åˆ¥
                brand_match = 0
                domain_parts = base_domain.split('.')
                # æ”¹é€²åŸŸåæå–é‚è¼¯ï¼šè™•ç†å¤šç´šåŸŸå
                if len(domain_parts) >= 2:
                    # å°æ–¼å¸¸è¦‹çš„é ‚ç´šåŸŸåï¼Œå–å€’æ•¸ç¬¬äºŒå€‹éƒ¨åˆ†
                    tld = domain_parts[-1]
                    if tld in ['com', 'org', 'net', 'edu', 'gov', 'mil']:
                        core_domain = domain_parts[-2]
                    else:
                        # å°æ–¼å…¶ä»–åŸŸåï¼Œå–å€’æ•¸ç¬¬ä¸‰å€‹éƒ¨åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        core_domain = domain_parts[-3] if len(domain_parts) >= 3 else domain_parts[-2]
                else:
                    core_domain = domain_parts[0]

                title_tag = soup.find('title')
                if title_tag and core_domain in title_tag.get_text().lower():
                    brand_match = 1
                elif soup.find('meta', attrs={'name': 'description'}) and core_domain in soup.find('meta', attrs={'name': 'description'})['content'].lower():
                    brand_match = 1
                features['domain_in_brand'] = brand_match


                # --- 3. nb_hyperlinks: ç¶²é ä¸­è¶…é€£çµçš„ç¸½æ•¸ ---
                all_links = soup.find_all('a', href=True)
                features['nb_hyperlinks'] = len(all_links)

                # --- 4. ratio_intHyperlinks: å…§éƒ¨è¶…é€£çµçš„æ¯”ä¾‹ ---
                # --- 5. ratio_extHyperlinks: å¤–éƒ¨è¶…é€£çµçš„æ¯”ä¾‹ ---
                internal_links = 0
                external_links = 0
                for link_tag in all_links:
                    href = link_tag['href']
                    full_url = urljoin(url, href)
                    linked_domain = urlparse(full_url).netloc
                    if linked_domain == parsed_url.netloc:
                        internal_links += 1
                    else:
                        external_links += 1
                total_links_calc = internal_links + external_links
                features['ratio_intHyperlinks'] = internal_links / total_links_calc if total_links_calc > 0 else 0.0
                features['ratio_extHyperlinks'] = external_links / total_links_calc if total_links_calc > 0 else 0.0


                # --- 6. ratio_extRedirection (å¤–éƒ¨é‡æ–°å°å‘çš„æ¯”ä¾‹) ---
                # æª¢æŸ¥å¤–éƒ¨é€£çµæ˜¯å¦åŒ…å«é‡å®šå‘ç›¸é—œçš„å±¬æ€§æˆ–JavaScript
                redirect_count = 0
                for link_tag in all_links:
                    href = link_tag['href']
                    if href.startswith('#'):
                        continue
                    full_url = urljoin(url, href)
                    linked_parsed = urlparse(full_url)
                    linked_domain = linked_parsed.netloc
                    is_external = (linked_domain != parsed_url.netloc)

                    if is_external:
                        # æª¢æŸ¥æ˜¯å¦æœ‰é‡å®šå‘ç›¸é—œçš„å±¬æ€§
                        if link_tag.get('onclick') and 'window.location' in link_tag.get('onclick', ''):
                            redirect_count += 1
                        elif link_tag.get('target') == '_blank' and 'redirect' in link_tag.get_text().lower():
                            redirect_count += 1

                features['ratio_extRedirection'] = redirect_count / len(all_links) if all_links else 0.0

                # --- 7. ratio_extErrors (å¤–éƒ¨é€£çµä¸­è¿”å›éŒ¯èª¤çš„æ¯”ä¾‹) ---
                # æª¢æŸ¥å¤–éƒ¨é€£çµæ˜¯å¦æŒ‡å‘æ˜é¡¯éŒ¯èª¤çš„URLæ ¼å¼
                error_count = 0
                for link_tag in all_links:
                    href = link_tag['href']
                    if href.startswith('#'):
                        continue
                    full_url = urljoin(url, href)
                    linked_parsed = urlparse(full_url)
                    linked_domain = linked_parsed.netloc
                    is_external = (linked_domain != parsed_url.netloc)

                    if is_external:
                        # æª¢æŸ¥æ˜¯å¦ç‚ºæ˜é¡¯éŒ¯èª¤çš„URL
                        if 'error' in full_url.lower() or '404' in full_url or 'notfound' in full_url.lower():
                            error_count += 1
                        elif not linked_domain or linked_domain == '':
                            error_count += 1

                features['ratio_extErrors'] = error_count / len(all_links) if all_links else 0.0

                # --- 8. external_favicon: ç¶²ç«™æ˜¯å¦ä½¿ç”¨ä¾†è‡ªå¤–éƒ¨åŸŸåçš„ Favicon ---
                favicon_link = soup.find('link', rel=lambda x: x and 'icon' in x.lower())
                features['external_favicon'] = 0.0
                if favicon_link and 'href' in favicon_link.attrs:
                    favicon_url = urljoin(url, favicon_link['href'])
                    favicon_domain = urlparse(favicon_url).netloc
                    if favicon_domain != parsed_url.netloc:
                        features['external_favicon'] = 1.0

                # --- 9. links_in_tags: ç‰¹å®š HTML æ¨™ç±¤ï¼ˆå¦‚ <a>ã€<script>ï¼‰ä¸­é€£çµçš„æ•¸é‡ ---
                # é€™è£¡çµ±è¨ˆæ‰€æœ‰ href å’Œ src å±¬æ€§çš„é€£çµ
                total_links_in_tags = 0
                for tag in soup.find_all(['a', 'script', 'img', 'link', 'iframe', 'form']):
                    if 'href' in tag.attrs:
                        total_links_in_tags += 1
                    if 'src' in tag.attrs:
                        total_links_in_tags += 1
                    if tag.name == 'form' and 'action' in tag.attrs:
                        total_links_in_tags += 1
                features['links_in_tags'] = total_links_in_tags

                # --- 10. ratio_extMedia: å¤–éƒ¨åª’é«”ï¼ˆåœ–ç‰‡ã€éŸ³é »ã€è¦–é »ï¼‰çš„æ¯”ä¾‹ ---
                media_tags = soup.find_all(['img', 'audio', 'video', 'source'])
                total_media = len(media_tags)
                external_media = 0
                for media_tag in media_tags:
                    src = media_tag.get('src') or media_tag.get('href')
                    if src:
                        media_url = urljoin(url, src)
                        media_domain = urlparse(media_url).netloc
                        if media_domain != parsed_url.netloc:
                            external_media += 1
                features['ratio_extMedia'] = external_media / total_media if total_media > 0 else 0.0

                # --- 11. safe_anchor: éŒ¨é»é€£çµæ˜¯å¦å®‰å…¨ï¼ˆä¾‹å¦‚é¿å…æŒ‡å‘å¯ç–‘å¤–éƒ¨ç¶²ç«™ï¼‰ ---
                # æª¢æŸ¥æ˜¯å¦æœ‰æŒ‡å‘ IP åœ°å€ã€ä¸å¸¸è¦‹å”è­°æˆ–å¯ç–‘åŸŸåçš„å¤–éƒ¨é€£çµ
                features['safe_anchor'] = 1.0
                suspicious_keywords = ['bit.ly', 'tinyurl', 'goo.gl', 't.co', 'fb.me', 'is.gd']

                for link_tag in all_links:
                    href = link_tag['href']
                    if href.startswith('#'):
                        continue
                    full_url = urljoin(url, href)
                    linked_parsed = urlparse(full_url)
                    linked_domain = linked_parsed.netloc
                    is_external = (linked_domain != parsed_url.netloc)

                    if is_external:
                        # æª¢æŸ¥æ˜¯å¦ç‚ºIPåœ°å€
                        try:
                            ipaddress.ip_address(linked_domain)
                            features['safe_anchor'] = 0.0
                            break
                        except ValueError:
                            pass

                        # æª¢æŸ¥å”è­°æ˜¯å¦å®‰å…¨
                        if linked_parsed.scheme not in ['http', 'https', '']:
                            features['safe_anchor'] = 0.0
                            break

                        # æª¢æŸ¥æ˜¯å¦ç‚ºå¯ç–‘çš„çŸ­ç¶²å€æœå‹™
                        if any(keyword in linked_domain.lower() for keyword in suspicious_keywords):
                            features['safe_anchor'] = 0.0
                            break

                # --- 12. empty_title: ç¶²é æ¨™é¡Œæ˜¯å¦ç‚ºç©º ---
                features['empty_title'] = 1.0 if not (soup.title and soup.title.string and soup.title.string.strip()) else 0.0

                # --- 13. domain_in_title: åŸŸåæ˜¯å¦å‡ºç¾åœ¨ç¶²é æ¨™é¡Œä¸­ ---
                features['domain_in_title'] = 0.0
                if soup.title and soup.title.string:
                    if base_domain in soup.title.string.lower():
                        features['domain_in_title'] = 1.0

                # --- 14. domain_with_copyright: ç¶²ç«™çš„ç‰ˆæ¬Šè³‡è¨Šä¸­æ˜¯å¦åŒ…å«åŸŸå ---
                features['domain_with_copyright'] = 0.0

                # æª¢æŸ¥ç‰ˆæ¬Šæ–‡æœ¬
                copyright_text = soup.find(text=re.compile(r'Â©|copyright', re.IGNORECASE))
                if copyright_text and base_domain in copyright_text.lower():
                    features['domain_with_copyright'] = 1.0

                # æª¢æŸ¥footerå€åŸŸ
                footer_tags = soup.find_all(['div', 'footer'], class_=re.compile(r'footer|copyright', re.IGNORECASE))
                for footer in footer_tags:
                    if base_domain in footer.get_text().lower():
                        features['domain_with_copyright'] = 1.0
                        break

                # æª¢æŸ¥æ‰€æœ‰åŒ…å«ç‰ˆæ¬Šç›¸é—œæ–‡å­—çš„æ¨™ç±¤
                copyright_tags = soup.find_all(text=re.compile(r'Â©|copyright|all rights reserved', re.IGNORECASE))
                for tag in copyright_tags:
                    if base_domain in tag.lower():
                        features['domain_with_copyright'] = 1.0
                        break

                features['feature_extracted'] = 1.0
            except Exception as e:
                print(f"è§£æ URL '{url}' çš„ HTML æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                features = {col: 0.0 for col in html_feature_columns}
                features['feature_extracted'] = 0.0
        else:
            print(f"æœªèƒ½ç²å– URL '{url}' çš„ HTML å…§å®¹ã€‚æ‰€æœ‰ HTML ç‰¹å¾µå°‡ç‚º 0.0ã€‚")
            features = {col: 0.0 for col in html_feature_columns}
            features['feature_extracted'] = 0.0
        results.append(features)
        time.sleep(0.5)

    features_df = pd.DataFrame(results, index=df.index)
    df_with_features = pd.concat([df, features_df], axis=1)

    return df_with_features

import signal

class TimeoutException(Exception):
    pass

def handler(signum, frame):
    raise TimeoutException()

def safe_process_url(url, max_time=30):
    signal.signal(signal.SIGALRM, handler)
    signal.alarm(max_time)  # è¨­å®šæœ€å¤šè™•ç† max_time ç§’
    try:
        html_content, has_js_redirect = get_html_content(url, timeout=20, max_retries=1)
        signal.alarm(0)  # æˆåŠŸå°±æ¸…æ‰ alarm
        return html_content, has_js_redirect
    except TimeoutException:
        print(f"â° URL {url} è¶…é {max_time} ç§’ï¼Œå¼·åˆ¶è·³é")
        return None, False
    except Exception as e:
        print(f"âŒ URL {url} è™•ç†å¤±æ•—: {e}")
        return None, False


# --- ç¯„ä¾‹ä½¿ç”¨ ---
if __name__ == "__main__":

    df = load_data()
    df = df[42000:52000]
    # å‘¼å«å‡½å¼å‰µå»ºæ–°ç‰¹å¾µ
    # æ³¨æ„: æœƒè‡ªå‹•å…ˆä½¿ç”¨ requestsï¼Œå¦‚æœå…§å®¹ç‚ºç©ºå‰‡ä½¿ç”¨ Selenium
    df_with_html_features = extract_html_features(df, url_column='url')
    # save_df(df_with_html_features, 'output_with_features_5000toEnd.csv')
    print("\nåŒ…å«æ–° HTML ç‰¹å¾µçš„ DataFrame:")
    print(df_with_html_features)

    print("\næ‰€æœ‰æ–°å‰µå»ºçš„ HTML ç‰¹å¾µæ¬„ä½åŠå…¶å€¼é¡å‹:")
    new_html_cols = [f for f in df_with_html_features.columns if f not in df.columns]
    print({col: df_with_html_features[col].dtype for col in new_html_cols})
